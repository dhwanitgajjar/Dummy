from flask import Flask, send_file, jsonify
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime, timedelta
import logging
import io

app = Flask(__name__)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_data():
    """
    Mock function simulating your load_data()
    Returns mock data objects
    """
    logger.info("Loading data from database/S3...")
    
    # Simulate data loading
    matters_data = "mock_matters_data"
    key_data = "mock_key_data"
    cumulative_query = "mock_cumulative_query"
    
    return matters_data, key_data, cumulative_query


def download_from_s3():
    """
    Mock function simulating S3 download (replaces upload_to_s3)
    """
    logger.info("Downloading from S3...")
    import time
    import random
    
    time.sleep(1)  # Simulate processing
    
    # Return True for success, False for failure
    return random.choice([True, True, True, False])  # 75% success rate


def generate_csv_file():
    """
    Background job to generate CSV periodically
    """
    logger.info("Background job: Generating CSV...")
    
    # Generate mock CSV data
    csv_content = "id,name,email,contract_date,value\n"
    for i in range(1000000):
        csv_content += f"{i},Contract_{i},user_{i}@example.com,2024-01-01,{i*1000}\n"
    
    logger.info("CSV generation complete")


@app.route('/download', methods=['GET', 'POST'])
def execute():
    logger.info("Process started")
    
    # Load data (simulating your load_data call)
    (matters_data, key_data, cumulative_query) = load_data()
    
    # Simulate saving data to CSV (like your to_csv calls)
    # In reality, these would write to temp files
    logger.info("Preparing CSV files...")
    
    # Simulate the download process
    result = download_from_s3()
    
    logger.info("Process End")
    
    if result == True:
        # Schedule background job
        scheduler = BackgroundScheduler()
        run_time = datetime.now() + timedelta(minutes=1)
        
        scheduler.add_job(
            generate_csv_file, 
            'interval', 
            minutes=15,
            id='second_job', 
            next_run_time=run_time
        )
        
        scheduler.start()
        
        # Generate and return CSV
        csv_data = io.StringIO()
        csv_data.write("id,name,email,contract_date,value\n")
        for i in range(100000):  # Smaller for quick response
            csv_data.write(f"{i},Contract_{i},user_{i}@example.com,2024-01-01,{i*1000}\n")
        
        # Convert to bytes
        csv_bytes = io.BytesIO(csv_data.getvalue().encode('utf-8'))
        csv_bytes.seek(0)
        
        return send_file(
            csv_bytes,
            mimetype='text/csv',
            as_attachment=True,
            download_name='contracts_download.csv'
        )
    else:
        return jsonify({"message": "Failed to download file"}), 500


if __name__ == '__main__':
    app.run(debug=True, port=5000, threaded=True)
